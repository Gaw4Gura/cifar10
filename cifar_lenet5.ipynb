{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss:  2.3012266159057617\n",
      "0 100 loss:  2.257014751434326\n",
      "0 200 loss:  2.0397090911865234\n",
      "0 300 loss:  1.806654691696167\n",
      "0 400 loss:  1.7996854782104492\n",
      "0 500 loss:  1.8516989946365356\n",
      "0 600 loss:  1.764099359512329\n",
      "0 700 loss:  1.7259016036987305\n",
      "0 accruacy:  0.4058\n",
      "1 0 loss:  1.752593994140625\n",
      "1 100 loss:  1.6205387115478516\n",
      "1 200 loss:  1.6674103736877441\n",
      "1 300 loss:  1.4940321445465088\n",
      "1 400 loss:  1.5181968212127686\n",
      "1 500 loss:  1.69584059715271\n",
      "1 600 loss:  1.5777965784072876\n",
      "1 700 loss:  1.3257876634597778\n",
      "1 accruacy:  0.455\n",
      "2 0 loss:  1.4320837259292603\n",
      "2 100 loss:  1.4230303764343262\n",
      "2 200 loss:  1.346543788909912\n",
      "2 300 loss:  1.5770606994628906\n",
      "2 400 loss:  1.5052869319915771\n",
      "2 500 loss:  1.6831152439117432\n",
      "2 600 loss:  1.5782365798950195\n",
      "2 700 loss:  1.4543124437332153\n",
      "2 accruacy:  0.4722\n",
      "3 0 loss:  1.7353651523590088\n",
      "3 100 loss:  1.6109898090362549\n",
      "3 200 loss:  1.2862412929534912\n",
      "3 300 loss:  1.4332849979400635\n",
      "3 400 loss:  1.5409505367279053\n",
      "3 500 loss:  1.4503920078277588\n",
      "3 600 loss:  1.4372318983078003\n",
      "3 700 loss:  1.23289954662323\n",
      "3 accruacy:  0.4971\n",
      "4 0 loss:  1.1822384595870972\n",
      "4 100 loss:  1.3000277280807495\n",
      "4 200 loss:  1.1592280864715576\n",
      "4 300 loss:  1.4246101379394531\n",
      "4 400 loss:  1.3697896003723145\n",
      "4 500 loss:  1.327387809753418\n",
      "4 600 loss:  1.359029769897461\n",
      "4 700 loss:  1.3938648700714111\n",
      "4 accruacy:  0.5131\n",
      "5 0 loss:  1.2954635620117188\n",
      "5 100 loss:  1.2967530488967896\n",
      "5 200 loss:  1.3603637218475342\n",
      "5 300 loss:  1.3583861589431763\n",
      "5 400 loss:  1.2720057964324951\n",
      "5 500 loss:  1.5926426649093628\n",
      "5 600 loss:  1.3242011070251465\n",
      "5 700 loss:  1.3953291177749634\n",
      "5 accruacy:  0.5234\n",
      "6 0 loss:  1.150376319885254\n",
      "6 100 loss:  1.2541117668151855\n",
      "6 200 loss:  1.296854019165039\n",
      "6 300 loss:  1.4641920328140259\n",
      "6 400 loss:  1.325167179107666\n",
      "6 500 loss:  1.4232559204101562\n",
      "6 600 loss:  1.2939505577087402\n",
      "6 700 loss:  1.3421497344970703\n",
      "6 accruacy:  0.5365\n",
      "7 0 loss:  1.312530279159546\n",
      "7 100 loss:  1.339726209640503\n",
      "7 200 loss:  1.2322421073913574\n",
      "7 300 loss:  1.4160714149475098\n",
      "7 400 loss:  1.2055470943450928\n",
      "7 500 loss:  1.322251319885254\n",
      "7 600 loss:  1.1139347553253174\n",
      "7 700 loss:  1.4591665267944336\n",
      "7 accruacy:  0.5405\n",
      "8 0 loss:  1.21142578125\n",
      "8 100 loss:  1.2605247497558594\n",
      "8 200 loss:  1.333514928817749\n",
      "8 300 loss:  1.2704663276672363\n",
      "8 400 loss:  1.4664316177368164\n",
      "8 500 loss:  1.4049837589263916\n",
      "8 600 loss:  1.1253225803375244\n",
      "8 700 loss:  1.167366623878479\n",
      "8 accruacy:  0.5422\n",
      "9 0 loss:  1.467529058456421\n",
      "9 100 loss:  1.309636116027832\n",
      "9 200 loss:  1.2465360164642334\n",
      "9 300 loss:  1.0047760009765625\n",
      "9 400 loss:  1.2350863218307495\n",
      "9 500 loss:  1.1577181816101074\n",
      "9 600 loss:  1.2705045938491821\n",
      "9 700 loss:  1.096381425857544\n",
      "9 accruacy:  0.5572\n",
      "10 0 loss:  1.2080756425857544\n",
      "10 100 loss:  1.177416443824768\n",
      "10 200 loss:  1.2757365703582764\n",
      "10 300 loss:  1.0828092098236084\n",
      "10 400 loss:  0.8598353862762451\n",
      "10 500 loss:  1.446187973022461\n",
      "10 600 loss:  1.230521559715271\n",
      "10 700 loss:  1.1866099834442139\n",
      "10 accruacy:  0.5571\n",
      "11 0 loss:  1.1935360431671143\n",
      "11 100 loss:  1.1064749956130981\n",
      "11 200 loss:  1.3338135480880737\n",
      "11 300 loss:  1.7722686529159546\n",
      "11 400 loss:  1.1482653617858887\n",
      "11 500 loss:  1.1357524394989014\n",
      "11 600 loss:  1.107358694076538\n",
      "11 700 loss:  1.043421983718872\n",
      "11 accruacy:  0.5617\n",
      "12 0 loss:  1.2839387655258179\n",
      "12 100 loss:  1.0620527267456055\n",
      "12 200 loss:  1.3893380165100098\n",
      "12 300 loss:  1.1291553974151611\n",
      "12 400 loss:  1.2874443531036377\n",
      "12 500 loss:  1.473395824432373\n",
      "12 600 loss:  1.35917329788208\n",
      "12 700 loss:  1.1533823013305664\n",
      "12 accruacy:  0.565\n",
      "13 0 loss:  1.2847847938537598\n",
      "13 100 loss:  1.0970298051834106\n",
      "13 200 loss:  1.1542694568634033\n",
      "13 300 loss:  1.0625840425491333\n",
      "13 400 loss:  1.1416397094726562\n",
      "13 500 loss:  1.4557942152023315\n",
      "13 600 loss:  1.038561463356018\n",
      "13 700 loss:  0.900481104850769\n",
      "13 accruacy:  0.5594\n",
      "14 0 loss:  1.086852788925171\n",
      "14 100 loss:  1.1279561519622803\n",
      "14 200 loss:  1.1866767406463623\n",
      "14 300 loss:  1.1716049909591675\n",
      "14 400 loss:  1.3004844188690186\n",
      "14 500 loss:  1.2136954069137573\n",
      "14 600 loss:  0.9369915127754211\n",
      "14 700 loss:  1.1566503047943115\n",
      "14 accruacy:  0.5761\n",
      "15 0 loss:  1.2399840354919434\n",
      "15 100 loss:  1.2824859619140625\n",
      "15 200 loss:  1.4147876501083374\n",
      "15 300 loss:  1.0255502462387085\n",
      "15 400 loss:  1.3159899711608887\n",
      "15 500 loss:  0.9925724267959595\n",
      "15 600 loss:  1.3854451179504395\n",
      "15 700 loss:  1.1770716905593872\n",
      "15 accruacy:  0.5704\n",
      "16 0 loss:  1.1175628900527954\n",
      "16 100 loss:  1.1663174629211426\n",
      "16 200 loss:  1.121121883392334\n",
      "16 300 loss:  1.0613458156585693\n",
      "16 400 loss:  0.8940205574035645\n",
      "16 500 loss:  1.0487319231033325\n",
      "16 600 loss:  1.2412794828414917\n",
      "16 700 loss:  1.2041471004486084\n",
      "16 accruacy:  0.585\n",
      "17 0 loss:  1.1264694929122925\n",
      "17 100 loss:  1.282326102256775\n",
      "17 200 loss:  1.1797153949737549\n",
      "17 300 loss:  0.9909172654151917\n",
      "17 400 loss:  0.9967527985572815\n",
      "17 500 loss:  1.0669891834259033\n",
      "17 600 loss:  1.072596549987793\n",
      "17 700 loss:  0.9376039505004883\n",
      "17 accruacy:  0.5753\n",
      "18 0 loss:  1.134113073348999\n",
      "18 100 loss:  1.1151854991912842\n",
      "18 200 loss:  1.1153641939163208\n",
      "18 300 loss:  1.232995867729187\n",
      "18 400 loss:  1.2386493682861328\n",
      "18 500 loss:  1.141825556755066\n",
      "18 600 loss:  0.9474791288375854\n",
      "18 700 loss:  1.0835237503051758\n",
      "18 accruacy:  0.5938\n",
      "19 0 loss:  1.1207998991012573\n",
      "19 100 loss:  0.9675623178482056\n",
      "19 200 loss:  1.1197545528411865\n",
      "19 300 loss:  0.8450931310653687\n",
      "19 400 loss:  1.073919415473938\n",
      "19 500 loss:  1.0288004875183105\n",
      "19 600 loss:  1.0062572956085205\n",
      "19 700 loss:  1.1755330562591553\n",
      "19 accruacy:  0.5934\n",
      "20 0 loss:  1.0545618534088135\n",
      "20 100 loss:  1.0153124332427979\n",
      "20 200 loss:  0.9207444190979004\n",
      "20 300 loss:  1.1543843746185303\n",
      "20 400 loss:  1.1678576469421387\n",
      "20 500 loss:  1.195585012435913\n",
      "20 600 loss:  0.9801093339920044\n",
      "20 700 loss:  0.9854820370674133\n",
      "20 accruacy:  0.5919\n",
      "21 0 loss:  0.8245913982391357\n",
      "21 100 loss:  0.807437539100647\n",
      "21 200 loss:  1.1233471632003784\n",
      "21 300 loss:  0.8903605341911316\n",
      "21 400 loss:  0.9989557266235352\n",
      "21 500 loss:  0.9743691682815552\n",
      "21 600 loss:  1.1482439041137695\n",
      "21 700 loss:  0.8751453161239624\n",
      "21 accruacy:  0.5999\n",
      "22 0 loss:  1.0705939531326294\n",
      "22 100 loss:  1.12930428981781\n",
      "22 200 loss:  1.2308170795440674\n",
      "22 300 loss:  0.9603472352027893\n",
      "22 400 loss:  0.9147077798843384\n",
      "22 500 loss:  1.0636787414550781\n",
      "22 600 loss:  0.778532862663269\n",
      "22 700 loss:  0.8023456335067749\n",
      "22 accruacy:  0.5973\n",
      "23 0 loss:  0.9030565023422241\n",
      "23 100 loss:  1.078173041343689\n",
      "23 200 loss:  0.7512387037277222\n",
      "23 300 loss:  1.0361344814300537\n",
      "23 400 loss:  0.770362377166748\n",
      "23 500 loss:  1.405656099319458\n",
      "23 600 loss:  0.9920262694358826\n",
      "23 700 loss:  0.9619487524032593\n",
      "23 accruacy:  0.5985\n",
      "24 0 loss:  1.1011985540390015\n",
      "24 100 loss:  1.025160312652588\n",
      "24 200 loss:  0.9847808480262756\n",
      "24 300 loss:  1.229964256286621\n",
      "24 400 loss:  0.9459680318832397\n",
      "24 500 loss:  0.9542431831359863\n",
      "24 600 loss:  0.9877416491508484\n",
      "24 700 loss:  0.8589122295379639\n",
      "24 accruacy:  0.6096\n",
      "25 0 loss:  1.1400617361068726\n",
      "25 100 loss:  1.0720946788787842\n",
      "25 200 loss:  0.995011568069458\n",
      "25 300 loss:  0.9336564540863037\n",
      "25 400 loss:  0.9757211208343506\n",
      "25 500 loss:  1.1260030269622803\n",
      "25 600 loss:  1.0628025531768799\n",
      "25 700 loss:  0.728335976600647\n",
      "25 accruacy:  0.6031\n",
      "26 0 loss:  0.9757229089736938\n",
      "26 100 loss:  1.1148287057876587\n",
      "26 200 loss:  1.0536673069000244\n",
      "26 300 loss:  1.085939645767212\n",
      "26 400 loss:  0.9994829893112183\n",
      "26 500 loss:  1.076291561126709\n",
      "26 600 loss:  0.8407928943634033\n",
      "26 700 loss:  0.9901078343391418\n",
      "26 accruacy:  0.6075\n",
      "27 0 loss:  0.7940356731414795\n",
      "27 100 loss:  0.9430307149887085\n",
      "27 200 loss:  0.9576596021652222\n",
      "27 300 loss:  0.9350906610488892\n",
      "27 400 loss:  1.0137512683868408\n",
      "27 500 loss:  1.016800880432129\n",
      "27 600 loss:  0.8981968760490417\n",
      "27 700 loss:  1.0271852016448975\n",
      "27 accruacy:  0.6099\n",
      "28 0 loss:  0.9746853709220886\n",
      "28 100 loss:  1.250526785850525\n",
      "28 200 loss:  1.0491173267364502\n",
      "28 300 loss:  0.9934123158454895\n",
      "28 400 loss:  0.9575473070144653\n",
      "28 500 loss:  0.950404942035675\n",
      "28 600 loss:  0.847103476524353\n",
      "28 700 loss:  0.7367792129516602\n",
      "28 accruacy:  0.6065\n",
      "29 0 loss:  1.0910820960998535\n",
      "29 100 loss:  1.0226751565933228\n",
      "29 200 loss:  0.9601519703865051\n",
      "29 300 loss:  0.8776644468307495\n",
      "29 400 loss:  1.1082649230957031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 500 loss:  0.7916538715362549\n",
      "29 600 loss:  0.8887718319892883\n",
      "29 700 loss:  0.991263747215271\n",
      "29 accruacy:  0.6012\n",
      "30 0 loss:  1.1337380409240723\n",
      "30 100 loss:  0.7779334783554077\n",
      "30 200 loss:  1.1077827215194702\n",
      "30 300 loss:  0.8515812754631042\n",
      "30 400 loss:  0.7062546014785767\n",
      "30 500 loss:  0.9437724947929382\n",
      "30 600 loss:  1.0530909299850464\n",
      "30 700 loss:  1.001492977142334\n",
      "30 accruacy:  0.6198\n",
      "31 0 loss:  1.0651981830596924\n",
      "31 100 loss:  0.9732367396354675\n",
      "31 200 loss:  1.0899953842163086\n",
      "31 300 loss:  0.7914661765098572\n",
      "31 400 loss:  0.6508108377456665\n",
      "31 500 loss:  1.1208500862121582\n",
      "31 600 loss:  0.9009143114089966\n",
      "31 700 loss:  0.8736617565155029\n",
      "31 accruacy:  0.6182\n",
      "32 0 loss:  0.9535546898841858\n",
      "32 100 loss:  0.9147692918777466\n",
      "32 200 loss:  0.9428251385688782\n",
      "32 300 loss:  1.1126493215560913\n",
      "32 400 loss:  0.9279395937919617\n",
      "32 500 loss:  0.9911870956420898\n",
      "32 600 loss:  0.7180837392807007\n",
      "32 700 loss:  0.9657282829284668\n",
      "32 accruacy:  0.6115\n",
      "33 0 loss:  0.9347472190856934\n",
      "33 100 loss:  0.8838146924972534\n",
      "33 200 loss:  0.8651447892189026\n",
      "33 300 loss:  1.3259233236312866\n",
      "33 400 loss:  0.8607787489891052\n",
      "33 500 loss:  1.165816068649292\n",
      "33 600 loss:  0.9099546670913696\n",
      "33 700 loss:  0.8875613808631897\n",
      "33 accruacy:  0.6171\n",
      "34 0 loss:  0.633313000202179\n",
      "34 100 loss:  0.9542173147201538\n",
      "34 200 loss:  0.8049249649047852\n",
      "34 300 loss:  0.9939693212509155\n",
      "34 400 loss:  1.0443446636199951\n",
      "34 500 loss:  1.1956160068511963\n",
      "34 600 loss:  1.064919114112854\n",
      "34 700 loss:  0.9003341794013977\n",
      "34 accruacy:  0.627\n",
      "35 0 loss:  0.972642183303833\n",
      "35 100 loss:  1.0000733137130737\n",
      "35 200 loss:  0.7736231088638306\n",
      "35 300 loss:  1.1076736450195312\n",
      "35 400 loss:  1.0230751037597656\n",
      "35 500 loss:  1.014064073562622\n",
      "35 600 loss:  0.9576883912086487\n",
      "35 700 loss:  0.9650819301605225\n",
      "35 accruacy:  0.6256\n",
      "36 0 loss:  0.7503594756126404\n",
      "36 100 loss:  0.814702570438385\n",
      "36 200 loss:  0.9873050451278687\n",
      "36 300 loss:  0.823587954044342\n",
      "36 400 loss:  0.9451301097869873\n",
      "36 500 loss:  1.1815106868743896\n",
      "36 600 loss:  0.9182687997817993\n",
      "36 700 loss:  0.8202388882637024\n",
      "36 accruacy:  0.6231\n",
      "37 0 loss:  0.9895464181900024\n",
      "37 100 loss:  0.7936640977859497\n",
      "37 200 loss:  0.8157853484153748\n",
      "37 300 loss:  1.0977449417114258\n",
      "37 400 loss:  0.9790858030319214\n",
      "37 500 loss:  0.907406210899353\n",
      "37 600 loss:  0.9413872957229614\n",
      "37 700 loss:  0.9876002073287964\n",
      "37 accruacy:  0.6269\n",
      "38 0 loss:  0.9439584016799927\n",
      "38 100 loss:  0.9034889936447144\n",
      "38 200 loss:  0.8680441379547119\n",
      "38 300 loss:  1.1184895038604736\n",
      "38 400 loss:  0.7658214569091797\n",
      "38 500 loss:  1.1752333641052246\n",
      "38 600 loss:  0.969560980796814\n",
      "38 700 loss:  0.8541481494903564\n",
      "38 accruacy:  0.6266\n",
      "39 0 loss:  0.8440650701522827\n",
      "39 100 loss:  0.8157625198364258\n",
      "39 200 loss:  0.9607269763946533\n",
      "39 300 loss:  0.6120818853378296\n",
      "39 400 loss:  0.6958749294281006\n",
      "39 500 loss:  1.1515133380889893\n",
      "39 600 loss:  0.7128186225891113\n",
      "39 700 loss:  1.1098074913024902\n",
      "39 accruacy:  0.6283\n",
      "40 0 loss:  0.9756637811660767\n",
      "40 100 loss:  0.7670309543609619\n",
      "40 200 loss:  1.0746092796325684\n",
      "40 300 loss:  0.8544917106628418\n",
      "40 400 loss:  0.6560150384902954\n",
      "40 500 loss:  0.9780471920967102\n",
      "40 600 loss:  0.7745130658149719\n",
      "40 700 loss:  1.1216614246368408\n",
      "40 accruacy:  0.6271\n",
      "41 0 loss:  0.8582960367202759\n",
      "41 100 loss:  1.0233615636825562\n",
      "41 200 loss:  1.1259407997131348\n",
      "41 300 loss:  0.9648629426956177\n",
      "41 400 loss:  0.9671587944030762\n",
      "41 500 loss:  0.9121021032333374\n",
      "41 600 loss:  0.9908139705657959\n",
      "41 700 loss:  0.990298867225647\n",
      "41 accruacy:  0.6249\n",
      "42 0 loss:  0.84736168384552\n",
      "42 100 loss:  0.861292839050293\n",
      "42 200 loss:  0.7967995405197144\n",
      "42 300 loss:  0.9705997705459595\n",
      "42 400 loss:  0.9624863862991333\n",
      "42 500 loss:  0.8919855952262878\n",
      "42 600 loss:  0.9430033564567566\n",
      "42 700 loss:  0.9896188974380493\n",
      "42 accruacy:  0.6264\n",
      "43 0 loss:  1.2811264991760254\n",
      "43 100 loss:  0.9518721103668213\n",
      "43 200 loss:  0.918389618396759\n",
      "43 300 loss:  1.0212986469268799\n",
      "43 400 loss:  0.9130075573921204\n",
      "43 500 loss:  0.5478222370147705\n",
      "43 600 loss:  0.7417256236076355\n",
      "43 700 loss:  0.8264071941375732\n",
      "43 accruacy:  0.626\n",
      "44 0 loss:  0.6319082975387573\n",
      "44 100 loss:  1.085085391998291\n",
      "44 200 loss:  0.8748598098754883\n",
      "44 300 loss:  0.9287765026092529\n",
      "44 400 loss:  0.8307719826698303\n",
      "44 500 loss:  0.8080491423606873\n",
      "44 600 loss:  0.9655219912528992\n",
      "44 700 loss:  0.8112163543701172\n",
      "44 accruacy:  0.6224\n",
      "45 0 loss:  0.7353528738021851\n",
      "45 100 loss:  0.7713949680328369\n",
      "45 200 loss:  0.9924517273902893\n",
      "45 300 loss:  0.7982349991798401\n",
      "45 400 loss:  0.6654804348945618\n",
      "45 500 loss:  0.8333512544631958\n",
      "45 600 loss:  0.9408363103866577\n",
      "45 700 loss:  0.5694400072097778\n",
      "45 accruacy:  0.6347\n",
      "46 0 loss:  0.700042188167572\n",
      "46 100 loss:  0.7106508016586304\n",
      "46 200 loss:  0.8589779138565063\n",
      "46 300 loss:  0.7775101661682129\n",
      "46 400 loss:  0.7056201696395874\n",
      "46 500 loss:  0.8569245934486389\n",
      "46 600 loss:  0.8234729766845703\n",
      "46 700 loss:  0.5871797800064087\n",
      "46 accruacy:  0.634\n",
      "47 0 loss:  0.9283825159072876\n",
      "47 100 loss:  0.9135434031486511\n",
      "47 200 loss:  0.9002327919006348\n",
      "47 300 loss:  0.988731861114502\n",
      "47 400 loss:  0.8522043228149414\n",
      "47 500 loss:  1.0769946575164795\n",
      "47 600 loss:  0.769169807434082\n",
      "47 700 loss:  0.7532602548599243\n",
      "47 accruacy:  0.631\n",
      "48 0 loss:  0.6404625177383423\n",
      "48 100 loss:  0.8953019380569458\n",
      "48 200 loss:  0.7737429141998291\n",
      "48 300 loss:  0.7114864587783813\n",
      "48 400 loss:  0.8915657997131348\n",
      "48 500 loss:  0.858431339263916\n",
      "48 600 loss:  0.6019938588142395\n",
      "48 700 loss:  0.8671061992645264\n",
      "48 accruacy:  0.6304\n",
      "49 0 loss:  0.7038520574569702\n",
      "49 100 loss:  0.6193222999572754\n",
      "49 200 loss:  0.9845686554908752\n",
      "49 300 loss:  0.7316051721572876\n",
      "49 400 loss:  0.7895638346672058\n",
      "49 500 loss:  0.8929036855697632\n",
      "49 600 loss:  0.9798476696014404\n",
      "49 700 loss:  0.9951953291893005\n",
      "49 accruacy:  0.6296\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "net = [\n",
    "    layers.Conv2D(6, kernel_size = [5, 5], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size = [2, 2], strides = 2, padding = \"same\"),\n",
    "    layers.Conv2D(16, kernel_size = [5, 5], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size = [2, 2], strides = 2, padding = \"same\"),\n",
    "]\n",
    "\n",
    "fc = [\n",
    "    layers.Dense(120, activation = tf.nn.relu),\n",
    "    layers.Dense(84, activation = tf.nn.relu),\n",
    "    layers.Dropout(rate = 0.5),\n",
    "    layers.Dense(10, activation = None),\n",
    "]\n",
    "\n",
    "def process(x, y):\n",
    "    x = tf.cast(x, dtype = tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype = tf.int32)\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "y_train = tf.squeeze(y_train, axis = 1)\n",
    "y_test = tf.squeeze(y_test, axis = 1)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(1000).map(process).batch(64)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.map(process).batch(64)\n",
    "\n",
    "def main():\n",
    "    lenet5 = Sequential(net)\n",
    "    fc_net = Sequential(fc)\n",
    "    \n",
    "    lenet5.build(input_shape = [None, 32, 32, 3])\n",
    "    fc_net.build(input_shape = [None, 1024])\n",
    "    optimizer = optimizers.Adam(lr = 1e-4)\n",
    "    \n",
    "    variables = lenet5.trainable_variables + fc_net.trainable_variables\n",
    "    for epoch in range(50):\n",
    "        for step, (x, y) in enumerate(train_data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                out = lenet5(x)\n",
    "                out = tf.reshape(out, [-1, 1024])\n",
    "                logits = fc_net(out)\n",
    "                y_onehot = tf.one_hot(y, depth = 10)\n",
    "                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits = True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, 'loss: ', float(loss))\n",
    "        \n",
    "        total = 0\n",
    "        ac = 0\n",
    "        for (x, y) in test_data:\n",
    "            out = lenet5(x)\n",
    "            out = tf.reshape(out, [-1, 1024])\n",
    "            logits = fc_net(out)\n",
    "            prob = tf.nn.softmax(logits, axis = 1)\n",
    "            pred = tf.argmax(prob, axis = 1)\n",
    "            pred = tf.cast(pred, dtype = tf.int32)\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype = tf.int32)\n",
    "            correct = tf.reduce_sum(correct)\n",
    "            total += x.shape[0]\n",
    "            ac += int(correct)\n",
    "        acc = ac / total\n",
    "        print(epoch, 'accruacy: ', acc)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **keras** to build a LeNet-5 model for cifar-10.  \n",
    "$input = [32, 32, 3]$  \n",
    "The first convolutional layer has $[5, 5, 6]$ kernels, using `SAME` convolution operation to get $[32, 32, 6]$ output  \n",
    "Then comes the first sampling layer with max pooling, which makes the output $[16, 16, 6]$  \n",
    "The second convolutional layer has $[5, 5, 16]$ kernels, using `SAME` convolution operation, and the output shape equals to $[16, 16, 16]$  \n",
    "And the second sampling layer  also use max pooling to reshape the output to $[8, 8, 16]$  \n",
    "Before the shape goes to the full connection layer we flatten it to a $[1, 8\\times 8\\times 16=1024]$ vector, and the first full connection layer has $120$ kernels while the second one has $84$ kernels.\n",
    "Using dropout operation can avoid overfitting problem. \n",
    "In the output layer, we use *softmax* regression:\n",
    "$$\n",
    "softmax(z_i)=\\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "to get the output for the $10$ output channels, and reduce the mean of cross entropy loss function:\n",
    "$$\n",
    "J(W, b) = -\\frac{1}{m}\\sum_{i=1}^{m} y^{(i)}\\ln\\hat{y}^{(i)}\n",
    "$$\n",
    "where $y^{(i)}$ is the standerd label and $\\hat{y}^{(i)}$ is the network's output.\n",
    "We can get an accuary around $63\\%$ after $50$ epochs with a mini-batch size of $64$."
   ]
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
