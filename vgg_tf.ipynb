{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss 2.3030176162719727\n",
      "0 100 loss 2.219264507293701\n",
      "0 200 loss 1.9556794166564941\n",
      "0 300 loss 1.956813097000122\n",
      "0 400 loss 1.8391170501708984\n",
      "0 500 loss 1.7429561614990234\n",
      "0 600 loss 1.6545085906982422\n",
      "0 700 loss 1.5768307447433472\n",
      "0 800 loss 1.4659579992294312\n",
      "0 900 loss 1.3519532680511475\n",
      "0 1000 loss 1.6038812398910522\n",
      "0 1100 loss 1.78470778465271\n",
      "0 1200 loss 1.2915844917297363\n",
      "0 1300 loss 1.5890097618103027\n",
      "0 1400 loss 1.097398281097412\n",
      "0 1500 loss 1.326195240020752\n",
      "0 accruacy:  0.5489\n",
      "1 0 loss 1.2197494506835938\n",
      "1 100 loss 1.2578954696655273\n",
      "1 200 loss 1.1089178323745728\n",
      "1 300 loss 1.5312414169311523\n",
      "1 400 loss 1.2294416427612305\n",
      "1 500 loss 1.0616861581802368\n",
      "1 600 loss 1.0130943059921265\n",
      "1 700 loss 0.9965970516204834\n",
      "1 800 loss 1.511429786682129\n",
      "1 900 loss 1.2414414882659912\n",
      "1 1000 loss 0.9643637537956238\n",
      "1 1100 loss 1.0630580186843872\n",
      "1 1200 loss 1.0899286270141602\n",
      "1 1300 loss 1.1711299419403076\n",
      "1 1400 loss 0.7736486196517944\n",
      "1 1500 loss 1.0905522108078003\n",
      "1 accruacy:  0.6214\n",
      "2 0 loss 1.3043566942214966\n",
      "2 100 loss 1.019885540008545\n",
      "2 200 loss 0.7724672555923462\n",
      "2 300 loss 0.6860472559928894\n",
      "2 400 loss 0.6678448915481567\n",
      "2 500 loss 0.8214960098266602\n",
      "2 600 loss 1.1582138538360596\n",
      "2 700 loss 0.5712124109268188\n",
      "2 800 loss 0.7603590488433838\n",
      "2 900 loss 0.6031597852706909\n",
      "2 1000 loss 0.8242782354354858\n",
      "2 1100 loss 0.7937108874320984\n",
      "2 1200 loss 0.7767771482467651\n",
      "2 1300 loss 0.7748264074325562\n",
      "2 1400 loss 0.7988228797912598\n",
      "2 1500 loss 0.8706495761871338\n",
      "2 accruacy:  0.6841\n",
      "3 0 loss 1.1688042879104614\n",
      "3 100 loss 0.8509311676025391\n",
      "3 200 loss 0.7162551879882812\n",
      "3 300 loss 0.9328108429908752\n",
      "3 400 loss 0.4886198043823242\n",
      "3 500 loss 0.8332080245018005\n",
      "3 600 loss 0.7225525379180908\n",
      "3 700 loss 0.7018224000930786\n",
      "3 800 loss 0.8113647699356079\n",
      "3 900 loss 0.6672223806381226\n",
      "3 1000 loss 0.8495862483978271\n",
      "3 1100 loss 0.6626954078674316\n",
      "3 1200 loss 0.8486040830612183\n",
      "3 1300 loss 0.5650413036346436\n",
      "3 1400 loss 0.5999789237976074\n",
      "3 1500 loss 0.49232998490333557\n",
      "3 accruacy:  0.7329\n",
      "4 0 loss 0.6024895906448364\n",
      "4 100 loss 0.466748982667923\n",
      "4 200 loss 0.4399588704109192\n",
      "4 300 loss 0.4276652932167053\n",
      "4 400 loss 0.6074063777923584\n",
      "4 500 loss 0.37922772765159607\n",
      "4 600 loss 0.5319175720214844\n",
      "4 700 loss 0.574010968208313\n",
      "4 800 loss 0.5462362766265869\n",
      "4 900 loss 0.8215696811676025\n",
      "4 1000 loss 0.6500742435455322\n",
      "4 1100 loss 0.7890839576721191\n",
      "4 1200 loss 0.7769713997840881\n",
      "4 1300 loss 0.4003637433052063\n",
      "4 1400 loss 0.45950937271118164\n",
      "4 1500 loss 0.6567858457565308\n",
      "4 accruacy:  0.7511\n",
      "5 0 loss 0.44752639532089233\n",
      "5 100 loss 0.6159412860870361\n",
      "5 200 loss 0.5323818922042847\n",
      "5 300 loss 0.440230131149292\n",
      "5 400 loss 0.8100056648254395\n",
      "5 500 loss 0.3491137623786926\n",
      "5 600 loss 0.32244500517845154\n",
      "5 700 loss 0.523129940032959\n",
      "5 800 loss 0.4250011146068573\n",
      "5 900 loss 0.4212903380393982\n",
      "5 1000 loss 0.566167950630188\n",
      "5 1100 loss 0.7977489829063416\n",
      "5 1200 loss 0.4468461871147156\n",
      "5 1300 loss 0.22868122160434723\n",
      "5 1400 loss 0.48175808787345886\n",
      "5 1500 loss 0.319862425327301\n",
      "5 accruacy:  0.7806\n",
      "6 0 loss 0.4557150602340698\n",
      "6 100 loss 0.516750693321228\n",
      "6 200 loss 0.5362759828567505\n",
      "6 300 loss 0.2434873878955841\n",
      "6 400 loss 0.5966614484786987\n",
      "6 500 loss 0.26538509130477905\n",
      "6 600 loss 0.48756957054138184\n",
      "6 700 loss 0.36031824350357056\n",
      "6 800 loss 0.3535473942756653\n",
      "6 900 loss 0.31115958094596863\n",
      "6 1000 loss 0.5746899843215942\n",
      "6 1100 loss 0.4492737054824829\n",
      "6 1200 loss 0.35522279143333435\n",
      "6 1300 loss 0.3155813217163086\n",
      "6 1400 loss 0.4501678943634033\n",
      "6 1500 loss 0.4595355987548828\n",
      "6 accruacy:  0.7806\n",
      "7 0 loss 0.3636772632598877\n",
      "7 100 loss 0.352799654006958\n",
      "7 200 loss 0.255622535943985\n",
      "7 300 loss 0.3831107020378113\n",
      "7 400 loss 0.33634835481643677\n",
      "7 500 loss 0.6403874158859253\n",
      "7 600 loss 0.6041437983512878\n",
      "7 700 loss 0.306806743144989\n",
      "7 800 loss 0.47151491045951843\n",
      "7 900 loss 0.2879149317741394\n",
      "7 1000 loss 0.3160959482192993\n",
      "7 1100 loss 0.5328906178474426\n",
      "7 1200 loss 0.24564436078071594\n",
      "7 1300 loss 0.2952493727207184\n",
      "7 1400 loss 0.487143874168396\n",
      "7 1500 loss 0.127413809299469\n",
      "7 accruacy:  0.8006\n",
      "8 0 loss 0.35699450969696045\n",
      "8 100 loss 0.4097431004047394\n",
      "8 200 loss 0.3092440962791443\n",
      "8 300 loss 0.43058156967163086\n",
      "8 400 loss 0.6001788377761841\n",
      "8 500 loss 0.23169414699077606\n",
      "8 600 loss 0.4929064214229584\n",
      "8 700 loss 0.3317413330078125\n",
      "8 800 loss 0.5001641511917114\n",
      "8 900 loss 0.4233555197715759\n",
      "8 1000 loss 0.5372583270072937\n",
      "8 1100 loss 0.19576124846935272\n",
      "8 1200 loss 0.15905891358852386\n",
      "8 1300 loss 0.1600256711244583\n",
      "8 1400 loss 0.1845240592956543\n",
      "8 1500 loss 0.2556471824645996\n",
      "8 accruacy:  0.8044\n",
      "9 0 loss 0.2678276300430298\n",
      "9 100 loss 0.4200327396392822\n",
      "9 200 loss 0.4256477355957031\n",
      "9 300 loss 0.1847197711467743\n",
      "9 400 loss 0.09941363334655762\n",
      "9 500 loss 0.24496889114379883\n",
      "9 600 loss 0.13885225355625153\n",
      "9 700 loss 0.1060447245836258\n",
      "9 800 loss 0.22798721492290497\n",
      "9 900 loss 0.09767329692840576\n",
      "9 1000 loss 0.10707805305719376\n",
      "9 1100 loss 0.1093079000711441\n",
      "9 1200 loss 0.3298514783382416\n",
      "9 1300 loss 0.23169493675231934\n",
      "9 1400 loss 0.08350520581007004\n",
      "9 1500 loss 0.048261821269989014\n",
      "9 accruacy:  0.8067\n",
      "10 0 loss 0.12817056477069855\n",
      "10 100 loss 0.21122117340564728\n",
      "10 200 loss 0.21643225848674774\n",
      "10 300 loss 0.10134410113096237\n",
      "10 400 loss 0.22940927743911743\n",
      "10 500 loss 0.15893253684043884\n",
      "10 600 loss 0.05872175842523575\n",
      "10 700 loss 0.31241756677627563\n",
      "10 800 loss 0.07686306536197662\n",
      "10 900 loss 0.19458884000778198\n",
      "10 1000 loss 0.15232457220554352\n",
      "10 1100 loss 0.13081364333629608\n",
      "10 1200 loss 0.09993404150009155\n",
      "10 1300 loss 0.1149817556142807\n",
      "10 1400 loss 0.10947103798389435\n",
      "10 1500 loss 0.035520654171705246\n",
      "10 accruacy:  0.7948\n",
      "11 0 loss 0.13787007331848145\n",
      "11 100 loss 0.23435591161251068\n",
      "11 200 loss 0.044537853449583054\n",
      "11 300 loss 0.12386374920606613\n",
      "11 400 loss 0.16155272722244263\n",
      "11 500 loss 0.04532808065414429\n",
      "11 600 loss 0.43688875436782837\n",
      "11 700 loss 0.12596574425697327\n",
      "11 800 loss 0.13161176443099976\n",
      "11 900 loss 0.19531142711639404\n",
      "11 1000 loss 0.0325806625187397\n",
      "11 1100 loss 0.04214392229914665\n",
      "11 1200 loss 0.24714793264865875\n",
      "11 1300 loss 0.020648835226893425\n",
      "11 1400 loss 0.03097427263855934\n",
      "11 1500 loss 0.04924391210079193\n",
      "11 accruacy:  0.7878\n",
      "12 0 loss 0.18997952342033386\n",
      "12 100 loss 0.0798921212553978\n",
      "12 200 loss 0.020526401698589325\n",
      "12 300 loss 0.17966991662979126\n",
      "12 400 loss 0.014299945905804634\n",
      "12 500 loss 0.10680973529815674\n",
      "12 600 loss 0.06959682703018188\n",
      "12 700 loss 0.10227543115615845\n",
      "12 800 loss 0.27044954895973206\n",
      "12 900 loss 0.010470164939761162\n",
      "12 1000 loss 0.14298906922340393\n",
      "12 1100 loss 0.01603824272751808\n",
      "12 1200 loss 0.0359036847949028\n",
      "12 1300 loss 0.021362535655498505\n",
      "12 1400 loss 0.05864745378494263\n",
      "12 1500 loss 0.035564493387937546\n",
      "12 accruacy:  0.806\n",
      "13 0 loss 0.07248011976480484\n",
      "13 100 loss 0.07114221900701523\n",
      "13 200 loss 0.002331727184355259\n",
      "13 300 loss 0.11259031295776367\n",
      "13 400 loss 0.01695258915424347\n",
      "13 500 loss 0.05024632439017296\n",
      "13 600 loss 0.04963520169258118\n",
      "13 700 loss 0.008682407438755035\n",
      "13 800 loss 0.3484106957912445\n",
      "13 900 loss 0.23210695385932922\n",
      "13 1000 loss 0.17802312970161438\n",
      "13 1100 loss 0.012695824727416039\n",
      "13 1200 loss 0.004125549923628569\n",
      "13 1300 loss 0.18629318475723267\n",
      "13 1400 loss 0.017085807397961617\n",
      "13 1500 loss 0.01186471339315176\n",
      "13 accruacy:  0.8084\n",
      "14 0 loss 0.04631023108959198\n",
      "14 100 loss 0.07177318632602692\n",
      "14 200 loss 0.02175658568739891\n",
      "14 300 loss 0.03807026147842407\n",
      "14 400 loss 0.008170710876584053\n",
      "14 500 loss 0.0220361165702343\n",
      "14 600 loss 0.04743649810552597\n",
      "14 700 loss 0.06129014492034912\n",
      "14 800 loss 0.012188931927084923\n",
      "14 900 loss 0.0015769621822983027\n",
      "14 1000 loss 0.02760191075503826\n",
      "14 1100 loss 0.020382480695843697\n",
      "14 1200 loss 0.032737795263528824\n",
      "14 1300 loss 0.06489668041467667\n",
      "14 1400 loss 0.006929418537765741\n",
      "14 1500 loss 0.08292154222726822\n",
      "14 accruacy:  0.7964\n",
      "15 0 loss 0.049023982137441635\n",
      "15 100 loss 0.05441625043749809\n",
      "15 200 loss 0.02641650289297104\n",
      "15 300 loss 0.10597467422485352\n",
      "15 400 loss 0.07124486565589905\n",
      "15 500 loss 0.03572097420692444\n",
      "15 600 loss 0.10094193369150162\n",
      "15 700 loss 0.015703924000263214\n",
      "15 800 loss 0.042365193367004395\n",
      "15 900 loss 0.002921414328739047\n",
      "15 1000 loss 0.02915497124195099\n",
      "15 1100 loss 0.00959860160946846\n",
      "15 1200 loss 0.0008206438506022096\n",
      "15 1300 loss 0.03362521529197693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 1400 loss 0.012278186157345772\n",
      "15 1500 loss 0.00714449817314744\n",
      "15 accruacy:  0.7964\n",
      "16 0 loss 0.015818465501070023\n",
      "16 100 loss 0.011301486752927303\n",
      "16 200 loss 0.20196859538555145\n",
      "16 300 loss 0.016271773725748062\n",
      "16 400 loss 0.10128629207611084\n",
      "16 500 loss 0.0038722832687199116\n",
      "16 600 loss 0.05524285510182381\n",
      "16 700 loss 0.031967051327228546\n",
      "16 800 loss 0.04056238383054733\n",
      "16 900 loss 0.03464455530047417\n",
      "16 1000 loss 0.11395113170146942\n",
      "16 1100 loss 0.006457194685935974\n",
      "16 1200 loss 0.07797442376613617\n",
      "16 1300 loss 0.058632005006074905\n",
      "16 1400 loss 0.006874356884509325\n",
      "16 1500 loss 0.025725437328219414\n",
      "16 accruacy:  0.7955\n",
      "17 0 loss 0.06164376810193062\n",
      "17 100 loss 0.16633255779743195\n",
      "17 200 loss 0.008514674380421638\n",
      "17 300 loss 0.010654594749212265\n",
      "17 400 loss 0.0017497382359579206\n",
      "17 500 loss 0.036298640072345734\n",
      "17 600 loss 0.11061179637908936\n",
      "17 700 loss 0.01228165440261364\n",
      "17 800 loss 0.0822848528623581\n",
      "17 900 loss 0.09501102566719055\n",
      "17 1000 loss 0.01109826285392046\n",
      "17 1100 loss 0.005473640281707048\n",
      "17 1200 loss 0.11031481623649597\n",
      "17 1300 loss 0.0184444822371006\n",
      "17 1400 loss 0.03892674297094345\n",
      "17 1500 loss 0.06610915809869766\n",
      "17 accruacy:  0.7986\n",
      "18 0 loss 0.23161396384239197\n",
      "18 100 loss 0.013242483139038086\n",
      "18 200 loss 0.09825898706912994\n",
      "18 300 loss 0.015474398620426655\n",
      "18 400 loss 0.03405165299773216\n",
      "18 500 loss 0.12767580151557922\n",
      "18 600 loss 0.007592714391648769\n",
      "18 700 loss 0.03530828654766083\n",
      "18 800 loss 0.05672092363238335\n",
      "18 900 loss 0.17808860540390015\n",
      "18 1000 loss 0.21376416087150574\n",
      "18 1100 loss 0.0409698411822319\n",
      "18 1200 loss 0.09009513258934021\n",
      "18 1300 loss 0.007494016550481319\n",
      "18 1400 loss 0.022533221170306206\n",
      "18 1500 loss 0.04553708806633949\n",
      "18 accruacy:  0.8104\n",
      "19 0 loss 0.028825975954532623\n",
      "19 100 loss 0.15283308923244476\n",
      "19 200 loss 0.14922846853733063\n",
      "19 300 loss 0.001351277227513492\n",
      "19 400 loss 0.19538207352161407\n",
      "19 500 loss 0.050051331520080566\n",
      "19 600 loss 0.001768444199115038\n",
      "19 700 loss 0.0010764007456600666\n",
      "19 800 loss 0.03777504712343216\n",
      "19 900 loss 0.006839060224592686\n",
      "19 1000 loss 0.08458553999662399\n",
      "19 1100 loss 0.02674066834151745\n",
      "19 1200 loss 0.004537851549685001\n",
      "19 1300 loss 0.0012830079067498446\n",
      "19 1400 loss 0.11117833852767944\n",
      "19 1500 loss 0.001254688249900937\n",
      "19 accruacy:  0.8023\n",
      "20 0 loss 0.010548711754381657\n",
      "20 100 loss 0.038505669683218\n",
      "20 200 loss 0.04091590270400047\n",
      "20 300 loss 0.04110034927725792\n",
      "20 400 loss 0.020603591576218605\n",
      "20 500 loss 0.0011409864528104663\n",
      "20 600 loss 0.06853152811527252\n",
      "20 700 loss 0.1071159765124321\n",
      "20 800 loss 0.31778863072395325\n",
      "20 900 loss 0.09507713466882706\n",
      "20 1000 loss 0.0010197159135714173\n",
      "20 1100 loss 0.009180926717817783\n",
      "20 1200 loss 0.12603867053985596\n",
      "20 1300 loss 0.046247467398643494\n",
      "20 1400 loss 0.00856159720569849\n",
      "20 1500 loss 0.035324856638908386\n",
      "20 accruacy:  0.8011\n",
      "21 0 loss 0.0014616334810853004\n",
      "21 100 loss 0.08524227142333984\n",
      "21 200 loss 0.001264180289581418\n",
      "21 300 loss 0.005994892213493586\n",
      "21 400 loss 0.011605852283537388\n",
      "21 500 loss 0.0028181641828268766\n",
      "21 600 loss 0.008460205048322678\n",
      "21 700 loss 0.004195512738078833\n",
      "21 800 loss 0.0076476954855024815\n",
      "21 900 loss 0.0022919545881450176\n",
      "21 1000 loss 0.0031353675294667482\n",
      "21 1100 loss 0.0037852502427995205\n",
      "21 1200 loss 0.2737543284893036\n",
      "21 1300 loss 0.017418012022972107\n",
      "21 1400 loss 0.04232005402445793\n",
      "21 1500 loss 0.006248270161449909\n",
      "21 accruacy:  0.8085\n",
      "22 0 loss 0.001545092323794961\n",
      "22 100 loss 0.0352129302918911\n",
      "22 200 loss 0.008665729314088821\n",
      "22 300 loss 0.045169346034526825\n",
      "22 400 loss 0.00886400043964386\n",
      "22 500 loss 0.01715359464287758\n",
      "22 600 loss 0.13959386944770813\n",
      "22 700 loss 0.014688308350741863\n",
      "22 800 loss 0.002157827839255333\n",
      "22 900 loss 0.002742381766438484\n",
      "22 1000 loss 0.16115008294582367\n",
      "22 1100 loss 0.0009959664894267917\n",
      "22 1200 loss 0.023104878142476082\n",
      "22 1300 loss 0.035232964903116226\n",
      "22 1400 loss 0.013217822648584843\n",
      "22 1500 loss 0.010940613225102425\n",
      "22 accruacy:  0.8071\n",
      "23 0 loss 0.02375703863799572\n",
      "23 100 loss 0.0004579326487146318\n",
      "23 200 loss 0.013331368565559387\n",
      "23 300 loss 0.006776975467801094\n",
      "23 400 loss 0.004384396597743034\n",
      "23 500 loss 0.05449298024177551\n",
      "23 600 loss 0.01061316579580307\n",
      "23 700 loss 0.00858204998075962\n",
      "23 800 loss 0.03560234233736992\n",
      "23 900 loss 0.012011159211397171\n",
      "23 1000 loss 0.004249453544616699\n",
      "23 1100 loss 0.00224971491843462\n",
      "23 1200 loss 0.0013945887330919504\n",
      "23 1300 loss 0.03532809391617775\n",
      "23 1400 loss 0.0053102788515388966\n",
      "23 1500 loss 0.0015220361528918147\n",
      "23 accruacy:  0.8127\n",
      "24 0 loss 0.02847897633910179\n",
      "24 100 loss 0.01157975010573864\n",
      "24 200 loss 0.17076188325881958\n",
      "24 300 loss 0.010391456075012684\n",
      "24 400 loss 0.12206382304430008\n",
      "24 500 loss 0.007162205874919891\n",
      "24 600 loss 0.0680927112698555\n",
      "24 700 loss 0.002159665571525693\n",
      "24 800 loss 0.00013391759421210736\n",
      "24 900 loss 0.006087776273488998\n",
      "24 1000 loss 0.06027046963572502\n",
      "24 1100 loss 0.15853579342365265\n",
      "24 1200 loss 0.0022667318116873503\n",
      "24 1300 loss 0.08336171507835388\n",
      "24 1400 loss 0.00028901948826387525\n",
      "24 1500 loss 0.003421216504648328\n",
      "24 accruacy:  0.8143\n",
      "25 0 loss 0.0013539614155888557\n",
      "25 100 loss 0.004958567675203085\n",
      "25 200 loss 0.012096927501261234\n",
      "25 300 loss 0.06281118839979172\n",
      "25 400 loss 0.10276587307453156\n",
      "25 500 loss 0.21541187167167664\n",
      "25 600 loss 0.0010663955472409725\n",
      "25 700 loss 0.008887735195457935\n",
      "25 800 loss 0.011606360785663128\n",
      "25 900 loss 0.002483350457623601\n",
      "25 1000 loss 0.08554136753082275\n",
      "25 1100 loss 0.0034202488604933023\n",
      "25 1200 loss 0.028353827074170113\n",
      "25 1300 loss 0.00015629263361915946\n",
      "25 1400 loss 0.0006596550811082125\n",
      "25 1500 loss 0.0019038351019844413\n",
      "25 accruacy:  0.8083\n",
      "26 0 loss 0.03737408295273781\n",
      "26 100 loss 0.00970542337745428\n",
      "26 200 loss 0.0054646749049425125\n",
      "26 300 loss 0.014188609085977077\n",
      "26 400 loss 0.007183725945651531\n",
      "26 500 loss 0.09700188040733337\n",
      "26 600 loss 0.21568629145622253\n",
      "26 700 loss 0.039612896740436554\n",
      "26 800 loss 0.08656396716833115\n",
      "26 900 loss 0.05014404281973839\n",
      "26 1000 loss 0.0017114157089963555\n",
      "26 1100 loss 0.03414004296064377\n",
      "26 1200 loss 0.00027010173653252423\n",
      "26 1300 loss 0.010832134634256363\n",
      "26 1400 loss 0.0018665723036974669\n",
      "26 1500 loss 0.0048128459602594376\n",
      "26 accruacy:  0.8101\n",
      "27 0 loss 0.042251523584127426\n",
      "27 100 loss 0.01065116748213768\n",
      "27 200 loss 0.017786523327231407\n",
      "27 300 loss 0.0008032845798879862\n",
      "27 400 loss 0.002573698293417692\n",
      "27 500 loss 0.05097118765115738\n",
      "27 600 loss 0.0021373219788074493\n",
      "27 700 loss 0.0315721295773983\n",
      "27 800 loss 0.35043907165527344\n",
      "27 900 loss 0.00010921458306256682\n",
      "27 1000 loss 0.010607284493744373\n",
      "27 1100 loss 0.034656964242458344\n",
      "27 1200 loss 0.02443346381187439\n",
      "27 1300 loss 0.005683376453816891\n",
      "27 1400 loss 0.0372534804046154\n",
      "27 1500 loss 0.0013370243832468987\n",
      "27 accruacy:  0.8089\n",
      "28 0 loss 0.00018775206990540028\n",
      "28 100 loss 0.10357408225536346\n",
      "28 200 loss 0.018324390053749084\n",
      "28 300 loss 0.002792800310999155\n",
      "28 400 loss 0.12587995827198029\n",
      "28 500 loss 0.0021232562139630318\n",
      "28 600 loss 0.006165287923067808\n",
      "28 700 loss 0.13808637857437134\n",
      "28 800 loss 0.0027406623121351004\n",
      "28 900 loss 0.001395984087139368\n",
      "28 1000 loss 0.0027746600098907948\n",
      "28 1100 loss 0.010228279046714306\n",
      "28 1200 loss 0.007677402347326279\n",
      "28 1300 loss 0.005868958774954081\n",
      "28 1400 loss 0.16111159324645996\n",
      "28 1500 loss 0.008500664494931698\n",
      "28 accruacy:  0.8046\n",
      "29 0 loss 0.02311946079134941\n",
      "29 100 loss 0.007011315785348415\n",
      "29 200 loss 0.0017562165157869458\n",
      "29 300 loss 0.08001259714365005\n",
      "29 400 loss 0.03809409216046333\n",
      "29 500 loss 0.0068509746342897415\n",
      "29 600 loss 0.10957477986812592\n",
      "29 700 loss 0.027018683031201363\n",
      "29 800 loss 0.0017530854092910886\n",
      "29 900 loss 0.08021091669797897\n",
      "29 1000 loss 0.0016746941255405545\n",
      "29 1100 loss 0.03916527330875397\n",
      "29 1200 loss 0.01521389465779066\n",
      "29 1300 loss 0.003716721897944808\n",
      "29 1400 loss 0.016706231981515884\n",
      "29 1500 loss 9.442777809454128e-05\n",
      "29 accruacy:  0.8148\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics, regularizers\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "vgg13 = [\n",
    "    layers.Conv2D(64, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size = [2, 2], strides = 2, padding = \"same\"),\n",
    "    \n",
    "    layers.Conv2D(128, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size = [2, 2], strides = 2, padding = \"same\"),\n",
    "    \n",
    "    layers.Conv2D(256, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    # layers.Conv2D(256, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu), \n",
    "    layers.MaxPool2D(pool_size = [2, 2], strides = 2, padding = \"same\"),\n",
    "    \n",
    "    layers.Conv2D(512, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    # layers.Conv2D(512, kernel_size = [3, 3], padding = \"same\", activation = tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size = [2, 2], strides = 2, padding = \"same\"),\n",
    "    \n",
    "]\n",
    "\n",
    "fc = [\n",
    "    layers.Dense(256, activation = tf.nn.relu),\n",
    "    layers.Dense(128, activation = tf.nn.relu),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation = None),\n",
    "]\n",
    "\n",
    "def process(x, y):\n",
    "    x = tf.cast(x, dtype = tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype = tf.int32)\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "y_train = tf.squeeze(y_train, axis = 1)\n",
    "y_test = tf.squeeze(y_test, axis = 1)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(1000).map(process).batch(32)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.map(process).batch(32)\n",
    "\n",
    "def main():\n",
    "    net = Sequential(vgg13)\n",
    "    fc_net = Sequential(fc)\n",
    "    \n",
    "    net.build(input_shape = [None, 32, 32, 3])\n",
    "    fc_net.build(input_shape = [None, 2048])\n",
    "    optimizer = optimizers.Adam(lr = 1e-4)\n",
    "    \n",
    "    variables = net.trainable_variables + fc_net.trainable_variables\n",
    "    for epoch in range(30):\n",
    "        for step, (x, y) in enumerate(train_data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                out = net(x)\n",
    "                out = tf.reshape(out, [-1, 2048])\n",
    "                logits = fc_net(out)\n",
    "                Y = tf.one_hot(y, depth = 10)\n",
    "                loss = tf.losses.categorical_crossentropy(Y, logits, from_logits = True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(epoch, step, 'loss', float(loss))\n",
    "    \n",
    "        total = 0\n",
    "        ac = 0\n",
    "        for (x, y) in test_data:\n",
    "            out = net(x)\n",
    "            out = tf.reshape(out, [-1, 2048])\n",
    "            logits = fc_net(out)\n",
    "            prob = tf.nn.softmax(logits, axis = 1)\n",
    "            pred = tf.argmax(prob, axis = 1)\n",
    "            pred = tf.cast(pred, dtype = tf.int32)\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype = tf.int32)\n",
    "            correct = tf.reduce_sum(correct)\n",
    "            total += x.shape[0]\n",
    "            ac += int(correct)\n",
    "        acc = ac / total\n",
    "        print(epoch, 'accruacy: ', acc)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
